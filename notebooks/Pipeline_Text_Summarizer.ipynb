{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac18271b",
   "metadata": {},
   "source": [
    " ## Model Selection: Research Paper Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fa95f",
   "metadata": {},
   "source": [
    "This model is by Janani Arunachalam and Kevin Thomas. Notebook can be found here: https://github.com/jananiarunachalam/Research-Paper-Summarization/blob/master/Abstractive_Summarization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b5a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm==4.36.1 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (4.36.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm==4.36.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0ad74b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rogue in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (0.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a9bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (1.2.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas) (1.17.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d37a60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "import glob\n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84a1ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Pickle'd file which has data stored in a dataframe\n",
    "# with headings: \"text\", \"filenames\", \"highlights\", \"body\"\n",
    "\n",
    "\n",
    "data = pd.read_pickle('/Users/sana/code/BertSumAbs/data/papers.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3163",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f869a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (3.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (1.17.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.36.1\n",
      "    Uninstalling tqdm-4.36.1:\n",
      "      Successfully uninstalled tqdm-4.36.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "simpletaxifare 1.0 requires numpy==1.18.5, but you have numpy 1.17.2 which is incompatible.\n",
      "simpletaxifare 1.0 requires six==1.15.0, but you have six 1.14.0 which is incompatible.\n",
      "taxifaremodel 1.0 requires numpy==1.18.5, but you have numpy 1.17.2 which is incompatible.\u001b[0m\n",
      "Successfully installed tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "236d21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    " \n",
    "def clean_body(text):\n",
    "    newText = text.lower()\n",
    "    newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
    "    newText = ' '.join(newText.split())\n",
    "    tokens = [w for w in newText.split() if not w in STOP_WORDS]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_body = []\n",
    "for t in data['body']:\n",
    "    cleaned_body.append(clean_body(t))\n",
    "\n",
    "#cleaned_body[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c6fe834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \"highlight\" text\n",
    "\n",
    "\n",
    "def clean_highlight(text):\n",
    "  newText = text.lower()\n",
    "  newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
    "  newText = ' '.join(newText.split())\n",
    "  newText = '_START_ '+ newText + ' _END_'\n",
    "  return newText\n",
    "\n",
    "cleaned_highlight = []\n",
    "for t in data['highlights']:\n",
    "    cleaned_highlight.append(clean_highlight(t))\n",
    "\n",
    "#cleaned_highlight[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a2a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing preprocessed data in the dataframe\n",
    "\n",
    "\n",
    "data['cleaned_highlights'] = cleaned_highlight\n",
    "data['cleaned_body'] = cleaned_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8485fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3a1856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5000 entries, 27 to 6950\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   text                5000 non-null   object\n",
      " 1   filenames           5000 non-null   object\n",
      " 2   highlights          5000 non-null   object\n",
      " 3   body                5000 non-null   object\n",
      " 4   cleaned_highlights  5000 non-null   object\n",
      " 5   cleaned_body        5000 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 273.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f9c364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "for i in data['cleaned_body']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_highlights']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'Body':text_word_count, 'Highlights':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a98454ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len_body = 1000\n",
    "max_len_highlight = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e12525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting cleaned data into strings\n",
    "\n",
    "\n",
    "data.cleaned_body = data.cleaned_body.apply(lambda x: str(x))\n",
    "data.cleaned_highlights = data.cleaned_highlights.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fec74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test sets\n",
    "# Test set is 20% of total data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(data['cleaned_body'],data['cleaned_highlights'],test_size=0.2,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "431239d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing \"body\"\n",
    "x_tok = Tokenizer()\n",
    "x_tok.fit_on_texts(list(x_train))\n",
    "\n",
    "# Converting text to number sequences\n",
    "x_train = x_tok.texts_to_sequences(x_train) \n",
    "x_test = x_tok.texts_to_sequences(x_test)\n",
    "\n",
    "# Padding zero upto maximum length\n",
    "x_train = pad_sequences(x_train,  maxlen=max_len_body, padding='post') \n",
    "x_test = pad_sequences(x_test, maxlen=max_len_body, padding='post')\n",
    "\n",
    "# Total number of words\n",
    "x_vocab_size = len(x_tok.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b14f5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing \"highlights\"\n",
    "y_tok = Tokenizer()\n",
    "y_tok.fit_on_texts(list(y_train))\n",
    "\n",
    "# Converting text to number sequences\n",
    "y_train = y_tok.texts_to_sequences(y_train) \n",
    "y_test = y_tok.texts_to_sequences(y_test)\n",
    "\n",
    "# Padding zero upto maximum length\n",
    "y_train = pad_sequences(y_train,  maxlen=max_len_highlight, padding='post') \n",
    "y_test = pad_sequences(y_test, maxlen=max_len_highlight, padding='post')\n",
    "\n",
    "# Word count\n",
    "y_vocab_size = len(y_tok.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eaa95",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b9250",
   "metadata": {},
   "source": [
    "\n",
    "Attention Layer required to remember context information. As there is no Keras implementation for Attention Layer we have found a third-party resource to aid in our model. Reference: https://github.com/thushv89/attention_keras/blob/master/layers/attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71f62d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac4683",
   "metadata": {},
   "source": [
    "\n",
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2af2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 14:43:34.132150: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1000, 50)     12730400    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 1000, 50), ( 20200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 1000, 50), ( 20200       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     819550      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 1000, 50), ( 20200       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 50), ( 5050        lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 100)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 16391)  1655491     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 15,291,291\n",
      "Trainable params: 15,291,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "\n",
    "K.clear_session() \n",
    "latent_dim = 50 \n",
    "\n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_body,)) \n",
    "enc_emb = Embedding(x_vocab_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "# 1st LSTM Layer\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "# 2nd LSTM Layer\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "# 3rd LSTM Layer\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Decoder \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_vocab_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "# LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "# Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# Model Definition\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0cfae",
   "metadata": {},
   "source": [
    "RMSProp was found to be the best optimizer for text summarization and sparse categorical crossentropy was found to be the best loss functiom from this reference: https://hackernoon.com/text-summarization-using-keras-models-366b002408d9\n",
    "Sparse categorical cross entropy is used as the loss function because it automatically converts an integer sequence to a one-hot encoded vector, hence requiring less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0389a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8756dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience = 3, monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc61a65",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5dc8ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f06686c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 49)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:,:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "912e4537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 49, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5c01ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 14:44:22.344587: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - 381s 12s/step - loss: 8.0085 - val_loss: 6.6266\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 322s 10s/step - loss: 6.8006 - val_loss: 6.4239\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 287s 9s/step - loss: 6.6769 - val_loss: 6.2732\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 351s 11s/step - loss: 6.5423 - val_loss: 6.1374\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 391s 12s/step - loss: 6.4434 - val_loss: 6.0567\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 337s 10s/step - loss: 6.3655 - val_loss: 5.9900\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 308s 10s/step - loss: 6.2972 - val_loss: 5.9391\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 322s 10s/step - loss: 6.2369 - val_loss: 5.9042\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 335s 10s/step - loss: 6.1875 - val_loss: 5.8734\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 351s 11s/step - loss: 6.1343 - val_loss: 5.8462\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 357s 11s/step - loss: 6.0882 - val_loss: 5.8276\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 342s 11s/step - loss: 6.0520 - val_loss: 5.7962\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 323s 10s/step - loss: 5.9943 - val_loss: 5.7838\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 333s 10s/step - loss: 5.9533 - val_loss: 5.7540\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 329s 10s/step - loss: 5.9179 - val_loss: 5.7711\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 321s 10s/step - loss: 5.8800 - val_loss: 5.7093\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 337s 11s/step - loss: 5.8423 - val_loss: 5.7007\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 344s 11s/step - loss: 5.8024 - val_loss: 5.6838\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 318s 10s/step - loss: 5.7706 - val_loss: 5.6501\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 324s 10s/step - loss: 5.7236 - val_loss: 5.6314\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 328s 10s/step - loss: 5.6839 - val_loss: 5.6034\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 332s 10s/step - loss: 5.6432 - val_loss: 5.5943\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 352s 11s/step - loss: 5.6091 - val_loss: 5.5607\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 319s 10s/step - loss: 5.5684 - val_loss: 5.5474\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 324s 10s/step - loss: 5.5295 - val_loss: 5.5324\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 313s 10s/step - loss: 5.4980 - val_loss: 5.5068\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 318s 10s/step - loss: 5.4611 - val_loss: 5.4924\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 314s 10s/step - loss: 5.4303 - val_loss: 5.4835\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 325s 10s/step - loss: 5.3923 - val_loss: 5.4827\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 313s 10s/step - loss: 5.3612 - val_loss: 5.4693\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train,y_train[:,:-1]], \n",
    "                    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n",
    "                    epochs=30, callbacks=[es],\n",
    "                    batch_size=128,\n",
    "                    validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17244244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sana/code/BertSumAbs'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e65c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 17:32:20.158377: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/sana/code/BertSumAbs/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/sana/code/BertSumAbs/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/sana/code/BertSumAbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ceb47543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing training ans test loss functions\n",
    "\n",
    "\n",
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e81c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tok.index_word \n",
    "reverse_source_word_index=x_tok.index_word \n",
    "target_word_index=y_tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5e4649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Inference\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder Inference\n",
    "# Below tensors hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_body,latent_dim))\n",
    "\n",
    "# Getting decoder sequence embeddings\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Predicting the next word in the sequence\n",
    "# Setting the initial states to the previous time step states\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# Attention Inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# Dense softmax layer to calculate probability distribution over target vocab\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final Decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ca60bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement inference\n",
    "\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encoding input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Taking the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        try:\n",
    "            sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        except:\n",
    "            sampled_token = reverse_target_word_index[np.random.randint(1, len(reverse_target_word_index))]\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_highlight-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7c943",
   "metadata": {},
   "source": [
    "Function to convert interger sequence to word sequence for highlights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f37be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2highlights(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5247e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a novel model for the health data we propose a novel model for the model of the model of the number of the number of the number of the number of the number of the number of the number of the number of the number of the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a new problem for the problem of the problem we propose a new problem for the problem of the problem we propose a new model for the problem of the problem we show the proposed approach to the new problem ggp cfd animals outofvocabulary multidea directions kirchhoff\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a new model for the model of the model of the model of the model of the model of the model of the model of the model of the model of the model distinguishing evaporating accept 111000 lysmer pz ease analysts tam walking epidemiologic confirms rewritten central\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a new problem for the problem of the problem we propose a new problem for the problem of the problem we propose a new problem for the problem of the problem we show the proposed approach to the new problem shadowed supplemental crfs disambiguation boosted autodesk mrf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a novel model for the model of the model of the model of the model of the model of the model of the model of the performance of the performance of the proposed method veoms misjudged programmed promise arabic 15 deviations automobile deltadelta uniting processed atannasovs sres\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a novel model for the model of the model of the model of the model of the model of the model of the model of the performance of the proposed method is used to the proposed method jointly chromosomal lotsizing pyrimidine seedinginspired decomposable link enumerating asp expresses\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a new model for the model of the model of the model of the model of the model of the model of the model of the model of the model converge frape hierarchies become lives automatically spreading hampered meanvariance iq unguided epilepsy postponement prepositioning ba rounded interaction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a novel model for the health of the health model we propose a novel model for the number of the binding of the binding of the binding of the binding of the binding of the binding of the binding model is used to be used to be\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a new model for the model of the model of the model of the model of the model of the model of the model of the model of the proposed method is proposed to the proposed method complexes nonredundant coarsegrained residuals usability mbd abaqus incentives numerical classimbalanced\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Highlights:\n",
      "\n",
      "\n",
      "Predicted summary:\n",
      " we propose a novel model for the model of the model of the model of the model of the model of the model of the model of the performance of the performance of the proposed method patients fmeasure farash presents lex automating lens watermark attractiveness gpcrdrug detected ologdnlogn sublevel\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference = []\n",
    "hypothesis = []\n",
    "for i in range(10):\n",
    "  print(\"Highlights:\")\n",
    "  #print(seq2summary(y_test[i]))\n",
    "  reference.append(seq2highlights(y_test[i]))\n",
    "  print(\"\\n\")\n",
    "  print(\"Predicted summary:\")\n",
    "  print(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
    "  hypothesis.append(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
    "  print(\"\\n\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798e5ff",
   "metadata": {},
   "source": [
    "## Calculating Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7997b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /Users/sana/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from rouge) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adf8474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "164a4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5412a0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.12351521683929426,\n",
       "  'p': 0.2090901116427432,\n",
       "  'f': 0.15309055254340953},\n",
       " 'rouge-2': {'r': 0.0232649476127737,\n",
       "  'p': 0.04382234432234432,\n",
       "  'f': 0.03031315692279345},\n",
       " 'rouge-l': {'r': 0.10470265520852776,\n",
       "  'p': 0.17769059011164273,\n",
       "  'f': 0.13006463998402845}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.get_scores(hypothesis, reference, avg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b52131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
